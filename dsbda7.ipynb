{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3e20d-2b73-4ead-b817-81f048864d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe4a56-74ee-4159-9ab1-c55ec7b87e3b",
   "metadata": {},
   "source": [
    "Download necessary resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d22284-e7a1-4123-95a2-cc79637452a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\VEDIKA/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VEDIKA/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VEDIKA/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VEDIKA/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9ab127-9e71-4e9e-b096-15dae20cb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) helps computers understand human language. It is widely used in sentiment analysis, chatbots, and text summarization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ff514-26e7-4b97-a7f9-1c86f2f8d6ae",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f28af3-5e95-4294-aa31-2cb4fe097f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing (NLP) helps computers understand human language.', 'It is widely used in sentiment analysis, chatbots, and text summarization.']\n",
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'helps', 'computers', 'understand', 'human', 'language', '.', 'it', 'is', 'widely', 'used', 'in', 'sentiment', 'analysis', ',', 'chatbots', ',', 'and', 'text', 'summarization', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences=sent_tokenize(text)\n",
    "words=word_tokenize(text.lower())\n",
    "print(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558e63f-103c-4d57-bbcf-449c1eb92ff9",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfdea7f-138c-4ab6-8903-a357ff4d93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'once', 'no', 'there', 'an', 'isn', 'them', \"they've\", 'through', 'how', 'yourself', 'don', 'mustn', 'very', 'of', 'up', 'd', \"i'd\", 'needn', 'yourselves', \"he'd\", 'm', 'more', \"weren't\", \"she's\", \"that'll\", 'some', 'but', 'that', 'been', 'further', 'didn', 'hasn', \"won't\", 'so', 'few', 'what', 'above', 'whom', \"she'd\", 'does', 'too', 'it', 'haven', 'just', 'again', 'between', \"shan't\", 'theirs', 've', 'wouldn', 'the', 'had', \"couldn't\", 'weren', \"haven't\", \"hasn't\", \"i'm\", 'below', 'not', \"they're\", 'won', 'and', 'before', 'other', 'because', 'couldn', \"he'll\", 'hadn', 're', \"wasn't\", 'your', \"i'll\", \"mustn't\", 'from', 'while', 'at', 'those', 'under', 'until', \"mightn't\", 'or', 'was', \"we'd\", 'now', 'itself', 'during', 'mightn', 'such', 'where', 'hers', 'he', 'you', 'she', 'on', 'than', 'down', 'ain', 'its', \"wouldn't\", 'if', 'in', \"you'd\", 'my', 'our', 'their', \"didn't\", 'only', 'wasn', 'who', \"she'll\", \"you're\", \"doesn't\", 'myself', 'did', \"he's\", 'they', \"isn't\", \"it's\", 'shan', 'about', 'any', \"needn't\", \"should've\", 'as', 'is', 's', 'against', 'has', 'ours', 'when', 'shouldn', 'themselves', 'his', 'these', 'a', 'am', 'most', 'being', 'over', \"you've\", 'nor', 'all', 'ourselves', \"we've\", \"it'd\", 'aren', \"they'll\", 'himself', 'can', 'have', \"we're\", 'this', 'by', 'why', 'are', 'same', 'with', \"i've\", \"shouldn't\", 'having', 'o', 'y', 'me', 'each', \"aren't\", 'here', \"don't\", 'him', 'into', 'out', 'ma', 'both', \"they'd\", \"you'll\", 'for', 'which', 'doesn', \"we'll\", 'be', 'doing', 'yours', 'we', 'off', 't', \"hadn't\", 'to', 'after', 'll', 'do', 'her', 'own', 'then', \"it'll\", 'will', 'i', 'herself', 'were', 'should'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8751a17-a8f3-488f-a7b6-21e00350d3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'nlp', 'helps', 'computers', 'understand', 'human', 'language', 'widely', 'used', 'sentiment', 'analysis', 'chatbots', 'text', 'summarization']\n"
     ]
    }
   ],
   "source": [
    "filtered_words=[word for word in words if word not in stop_words and word.isalpha()]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4adbd-c3a1-4f9a-984e-202c88c046fa",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa6ced8-8eec-4661-93c4-94603ff3cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'help', 'comput', 'understand', 'human', 'languag', 'wide', 'use', 'sentiment', 'analysi', 'chatbot', 'text', 'summar']\n",
      "['natural', 'language', 'processing', 'nlp', 'help', 'computer', 'understand', 'human', 'language', 'widely', 'used', 'sentiment', 'analysis', 'chatbots', 'text', 'summarization']\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmed_words=[ps.stem(word) for word in filtered_words]\n",
    "lemmatize_words=[lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(stemmed_words)\n",
    "print(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552aa245-257e-428d-9682-912cbebe8f99",
   "metadata": {},
   "source": [
    "Part of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f666556-0f62-4122-b69b-d5ffe485ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('nlp', 'NN'), ('helps', 'VBZ'), ('computers', 'NNS'), ('understand', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('widely', 'RB'), ('used', 'VBN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('chatbots', 'NNS'), ('text', 'JJ'), ('summarization', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags=nltk.pos_tag(filtered_words)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a57f0a-f755-46fb-9fe2-eb27f91efa35",
   "metadata": {},
   "source": [
    "TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b56c565a-b606-474b-90e3-b8361f5bbc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[1.]]\n"
     ]
    }
   ],
   "source": [
    "corpus=[''.join(lemmatize_words)]\n",
    "tfidf=TfidfVectorizer()\n",
    "X=tfidf.fit_transform(corpus).toarray()\n",
    "print(\"TF-IDF Matrix:\\n\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa432ce-6c77-40b0-8b01-ecda4866a62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
